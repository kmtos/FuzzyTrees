###########################
####### Description #######
###########################

Decision trees suffer from easily overfitting. There are several solutions to remedy this, but here I address two.

Boosting: Boosting takes many different weak learners and combines them to make a successful, and non-Overfit learning algorithm. The fundamental idea is that each iteration of the weak learners, you weight the training data points that got wrong more heavily, and the ones you get correct less heavily. The idea is that you want your learners to spend less "effort" correctly identifying the points you always get correct, and spend more "time" differentiating the points that you only get correct some of the time. Several notes: 

1) The weak learners must do better than random chance. 

2) If the weak learner overfits, the boosted conglomerate algorithm will overfit. 

3) I implement a basic Ada Boost. This one can run into problems if your data has noise. There are other boosts (a basic one is Brown Boost). Solutions help this by setting aside a certain percent of the incorrectly identified points to not weight or ignore. It basically assumes noise, but I believe (Haven't done much research so don't take my opinion by itself) that it should still work with or without noise when using algorithms like Brown Boost

Fuzzy Trees: The method here is to take the points that are near the cut point at nodes splitting into daughters, and give them a partial membership in both nodes. If you cut on a value and the extreme values have a much higher weight of a specific class value, then the points on the cusp of being on the other node than their current position, get grouped with points that are biased towards a different class value. By giving them partial membership, or more specifically, partial weights on both nodes, you can cfollow the point down the tree and get a weighted value for the group of end/decision nodes it reaches. 

In my implementation here, you can run a standard decision tree, a boosted decision tree, a fuzzy Classification of a regular tree, a fuzzy classification of a built fuzzy tree, a fuzzy classification of a boosted decision tree, or a fuzzy classification of a boosted fuzzy tree.

Sources:

Decision Tree: 1) https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/

Boosting: 1) https://storage.googleapis.com/supplemental_media/udacityu/367378584/Intro%20to%20Boosting.pdf,   2)https://www.cs.princeton.edu/courses/archive/spring07/cos424/papers/boosting-survey.pdf,   3)https://classroom.udacity.com/courses/ud262/lessons/367378584/concepts/3675486130923

Fuzzy Trees: 1) http://www.cs.ubbcluj.ro/~gabis/DocDiplome/DT/FuzzyDT/fuzzy

#################################
######## Implementation #########
#################################
FIRST NOTE: I like output. A lot of output. So scattered throughout this code lies many, many print statements. If you don't wantthe output, then comment it out. I like it so that I can quickly scan through it and verify that it is doing what I want it to do.
SECOND NOTE: Any variable that has "FileName" should have the ".csv" left off of it. The functions need the names in that form.

############## FILES ##############

DecisionTree.py: 

"MakeTreeOld" is the function that you must call to actually make the tree, which is a classification tree only so far. I DO NOT RECOMMEND THIS FUNCTION FOR AN ORDINARY DECISION TREE. There is another one which is faster, uses less memory, and is more easily personalized to whatever you want to do with it. Here are a description of the variables: "df" dataframe of training data. "className" is the name of the class variable you want to predict.  "nGiniSplits" the number of splits within a continuous variable to find the highest gini increase for a new node. "giniEndVal" the minimum value of gini increase to make a new node. This is to prevent further nodes that aren't needed as they don't add much. "maxDepth" is the maximum depth of a tree allowed. "idColumn" is the name of the column that identifies the points. "minSamplesSplit" the minimum number of points to allow a splitting into new nodes. "df_weights" is a dataframe of the idColumn and a column called "Weights" that you must create. It is the weight for each point. Make Tree won't weight the points unless used via a Boost. "nodeDFIDsFileName" is the name you want for the output of the dataframe's ID's, which keeps track of which points are in which node. "nodeValuesFileName" is the name of the splits at each node. "nodeDecisionsFileName" is the name of the file that you want for the output of the class value decisions at each node.

"FindingBestSplit"  is the function used to find the best split. This code is easily modified to include whatever methods you would like for finding the best split. The only requirement is that the output be the EXACT same in form and quantitative value for both this function, and the one it uses called "CalcBestGiniSplit", which can also be modified to calculate the best split however you desire.

"GetNodeDecisions" is a function that determines the decision at each decision node (EndNode, a node that is at max depth, or a node that has 1-2 blank nodes as daughters, for whatever reason) based upon pure democratic majority vote.

"GetWeightedNodeDecisions" is the same as the above function, except that it takes into account the points' weights from "df_weights".

"ClassifyWithTree" takes a test dataframe and uses it to classify points based upon an already created tree with training data. Here are a description of the variables: "df_test" is the test dataframe to be classified. "className" is the name of the class vacolumn variable to be predicted by this decision tree. "idColumn" is the name of the columnn in the dataframe that ID's the points. "maxDepth" is the max depth of the tree, which MUST be the same as the depth used for building the tree on the training data. Will NOT work otherwise. "outputFileName" is the name of the file that you want the points to be written to. Only writes out idColumn and className data. "nodeDecisionsFileName" is the name of the decisions of the built tree. MUST be the same as the one used in the construction of the tree. "nodeValuesFileName" is the name of the split values of each node as you built it. MUST also be the same as the name used in the construction of the tree.


Boosting.py:

"GetBoostingTreesErrorsAndWeights": This is the function used to implement the AdaBoost boosting algorithm. It uses "gini" instead of "entropy" to decide the node splits, but a self written function can easily be written in to use entropy. It uses the functions in "DecisionTree.py to actually make the individual trees. It is constructed such that if a run is ended prematurely, you can still start from where you left off and it'll keep track of everything. It counts down from the last estimator to make it easier to start off from where you last ended. Here are the descriptions of the variables. "df" is the training  dataframe. "nEstimators" is the number of estimators/decision trees you want to create to make the Boosting classification. "rateOfChange" is the variable that controls how quickly the weights change, so that misclassified points are weighted more heavily. 'd recommend a value < .5 and > 0 to control how quickly the weights can change. "df_weights" is the dataframe that must have the idColumn column and a column called "Weights" that is initialized to whatever values you desire. It holds and keeps track of the weights. "paramDict" is a dictionary that contains parameters for the "MakeFuzzyTree" function that is described later. Look at my example in TestTheFuzzy/testTheFuzzy.py for how to create. "colRandomness" is the fraction of randomly selected columns of the dataframe to included in each estimator. This helps fight against creating copies of the same tree in each estimator. "rowRandomness" is the fraction of randomly selected rows of the dataframe. This also helps each estimator to be more different. "treeErrorFileName" is the name of the file that writes out the error of each tree. This is used to weight the test points classification by how correct the tree is on the training data. "middleStart=False" is a boolean that dictates whether you are starting a run from scratch, or if, for some reason or another, the run ended early and you want to continue where you left off. This is also done so you can run with many estimators, but don't have to run them all in one shot. Just change this to "True" and put the number after the last completed estimator in the variable named "middleStartNum", and start over. "middleStartNum=-1" is used if you want to continue a run from where you left off. Set this variable to the next nEstimator value after the one you completed.

"GetTreeError" returns the error of the tree. This function can be substituted for another if you want to calculate the error using any other method, but the output should be the same. Also, make sure that the error is between 0 -> .5 or this algorithm fails.

"AlterWeights" alters the training points's weights based upon whether they were classified correctly. It used the error, where the more error a tree has, the less the weight will be changed, to change the values appropriately. This function can be modified or substituted for whatever method you want. It is easily changed to what you want.

"CalssifyWithBoost": This function classifies test points based upon the estimators and their errors. It writes out two files of the classification, one that only includes the idColumn and the className column, and the other that adds a bunch of extra data that includes the percent of belief in their respective class values. This function should with as many className unique values as possible. Here are the descriptions of the variables: "df_test" is the dataframe of test points. "nEstimators" is the number of estimators used in building the tree on the training data. "maxDepth" is the maximum depth of the trees used in the building of the tree. "idColumn" is the name of the column used to ID the points in the dataframe. "className" is the name of the column in the dataframe that you want to predict. "uniqueClasses" needs to be a list of possible classValues in the classification. Any values left out will have zero points classified with that value. "treeErrorFileName" is the name of the file that includes the errors of the trees in the building of the tree. "nodeValuesFileName" is the name of the file that has all the decisions of the nodes of each of the estimators. "nodeDecisionsFileName" is the name of the files that include the decisions of the nodes in the construction of the trees. "boostAnswersFileName" is the name of the output file that will have the predicted class values of each point.

FuzzyTrees_Classification.py:

"ClassifyWithFuzzyTree" is the function that classifies points based upon a regular decision tree already built in a matter that takes into account fuzzy memberships. The paper listed above describes what that means. This function is only concerned with classifying test points. Here are the descriptions of the variables: "df_test" is the dataframe of the test points. "className" is the name of the predictive class column in the dataframe. "idColumn" is the name of the dataframe column that identifies the points. "maxDepth" is the max depth of the previously constructed tree. "duality" is a factor > 0 that is multipled by the fuzzy membership range to vary how accepting you want the range to be. "uniqueClasses" is a list of the unique values of the className dataframe column. Any values not included will have no test points classified under it. "outputFileName" this is the name of the file that you want to output written to. "nodeDecisionsFileName" is the name of the decision file created in the construction of the tree. "nodeValuesFileName" is the name of the file that contains the splits of the already constructed tree.

"FuzzyMembershipLinear" calculates the membership of a point based upon its linear distance from the cut value and the "duality".

"FuzzyDecisionScoreUpdate" updates the score of value when classifying a point by taking its current value before updating and adding to it the value of the membership in the node that is being considered.

"BoostedFuzzyDecisionScoreUpdate" updates the same way as the "FuzzyDecisionScoreUpdate" except that it takes into account the "alpha" which changes based on the error of each estimator.

"FuzzyUpdateMembershipNodeList" updates the 'MembershipNodeList' dataframe column based upon each points current 'Memberships'.

"ClassifyWithBoostedFuzzyTree" is the same as "ClassifyWithFuzzyTree" except it uses an already constructed boosted set of Decision Trees. It has a few additional variables described here: "nEstimators" is the number of estimators used in the construction of the Decision tree. "treeErrorFileName" is the name of the file that contains all of the errors of the estimators.


FuzzyTrees_Building.py:

"MakeFuzzyTree" is the function that builds a Fuzzy Tree from training data. It creates two columns called 'Memberships', which keeps track of the nodes that each point is included in and its membership fraction in the form of a list of 2 item tuples, and 'MembershipNodeList', which is a list of the nodes that points are a part of. I needed both for some of the coding issues I ran into. Here are descriptions of the variables: "df" is the training dataframe, "className" is the name of the dataframe column that you want to predict on the test dataset. "nGiniSplits" is the number of splits used in continous data to find the split that maximizes the gini increase. "giniEndVal" is the minimum value of gini increase needed to merit a set of new nodes. "maxDepth" is the max depth of the Fuzzy Tree. "idColumn" is the name of the column in the dataframe that ID's the points. "minSamplesSplit" is the minimum number of points needed at a node to further split it. "duality" is the value which can shorten of lengthen the range that considers fuzzy memberships. If you want a regular tree that isn't fuzzy, then set this value to 0. "df_weights" is the dataframe which contains the idColumn and a column called 'Weights' that is the weight of each point. "nodeValuesFileName" is the name that you want for the file that has all the split values of each node. "nodeDecisionsFileName" is the name that you want for the file that has all the decisions for the nodes that need decisions.

"GetCurrentNodeFuzziedWeight" sets the intial memberships to be what the weight is in "df_weights".

"ReturnNodePointsToParent" is used when a daughter node is a blank node and the parent node is not an end node or a blank node. This happens when a node's daughter doesn't have enough points to split into further nodes or if the increase in gini is not enough to further split the node. All the points memberships are returned to the parent.

"GetFuzzyWeightedNodeDecisions" is the function that gets the decision of each node and takes into account the memberships of the nodes. 

"GetFuzzyBoostingTreesErrorsAndWeights"  is the function that builds a boosted set of Fuzzy Trees. Each estimator's membership is initialized by its current weight. This algorithm can be ended before finished and then continued from where left off. The variables 'middleStart' and 'middleStartNum' dictate starting from the middle of a run vs starting from the beginning. Here are descriptions of the variables: "df" is the training dataframe. "nEstimators" is the number of Fuzzy Trees to include in the boosted set. "rateOfChange" alters the changes in the weights after each estimator. "df_weights" is a dataframe that has two columns: the idColumn and a column called 'Weights' that contains the weights of each point. "paramDict" is a dictionary that contains parameters for the "MakeFuzzyTree" function that is described later. Look at my example in TestTheFuzzy/testTheFuzzy.py for how to create. "colRandomness" is the fraction of randomly selected columns of the dataframe to includedin each estimator. This helps fight against creating copies of the same tree in each estimator. "rowRandomness" is the fraction of randomly selected rows of the dataframe. This also helps each estimator to be more different. "treeErrorFileName" is the name of the file that writes out the error of each tree. This is used to weight the test points classification by how correct the tree is on the training data. "middleStart=False" is a boolean that dictates whether you are starting a run from scratch, or if, for some reason or another, the run ended early and you want to continue where you left off. This is also done so you can run with many estimators, but don't have to run them all in one shot. Just change this to "True" and put the number after the last completed estimator in the variable named "middleStartNum", and start over. "middleStartNum=-1" is used if you want to continue a run from where you left off. Set this variable to the next nEstimator value after the one you completed.

"GetTreeError" is the same as the previous definition in 'Boosting.py'. For some reason I had trouble with importing functions and could not get this to be imported, which I'm guessing is due to a mistake in importing in files that are imported in this file as well as importing that file directly. Anyways, this function gets the error of a tree based upon the weights of the points it got correct and incorrect.

"AlterFuzzyWeights" alters the weights of the points after each completed estimator based upon the points 'Memeberhips' column. So, each point could have a fraction of it correctly identified and a fraction mis-identified based upon which parts of the membership were correctly and incorrectly identified.
